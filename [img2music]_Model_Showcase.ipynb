{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Showcase\n",
        "`Img2Music` is a model that takes in an album cover image, and generates a music snippet based on the input image's features.\n",
        "It does this by predicting the most likely genre of the input album cover image, then generating an audio snippet given the predicted genre and the image's features.\n",
        "\n",
        "\n",
        "In this notebook, we will:\n",
        "1. initialize our datasets\n",
        "2. declare and initialize our model using `PyTorch`\n",
        "3. see what the model outputs when we pass in:\n",
        "  1. album cover images from the dataset\n",
        "  2. custom images\n",
        "\n",
        "** **\n",
        "**Note** that we simply showcasing the model's architecture and capabilities in this notebook.\n",
        "For the full training pipeline, see the included notebook `[img2music] Full Training Pipeline.ipynb`."
      ],
      "metadata": {
        "id": "cwYDoqkUHdD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "K_RMXDGpTfTH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgGvxgrmseTp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Imports & Pre-trained Weights downloads\n",
        "# @markdown Note that the pre-trained weights are obtained from the training pipeline and saved in google drive.\n",
        "# @markdown For more information on how the models were trained, see the `Full Training Pipeline.ipynb` file.\n",
        "\n",
        "%pip install torch-summary\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import pathlib\n",
        "import librosa\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms.v2 import GaussianNoise\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, Subset\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "os.makedirs(\"/content/pretrained_weights/\", exist_ok=True)\n",
        "!gdown 1cxWq874GMY0-Fb-dLVOiVn8zNknHBiTc --output \"/content/pretrained_weights/\"\n",
        "!gdown 1-9RjQlp8e4p9sejV7J_Sfin6fVhET7Ue --output \"/content/pretrained_weights/\"\n",
        "!gdown 1AUZ_TZ8ZD_-LmmGXAWR8NKG4rjrHd-12 --output \"/content/pretrained_weights/\"\n",
        "!gdown 1C1e2nuEQ05CQeWHrson4fFt6GCSlOk98 --output \"/content/pretrained_weights/\"\n",
        "!gdown 1-haiRrcriPoVolAU7o5rylNXELVUZE0T --output \"/content/pretrained_weights/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Declare Helper Functions\n",
        "def get_class_subsets(dataset: datasets.ImageFolder, class_names: list[str])  -> dict[str, Subset]:\n",
        "    \"\"\"\n",
        "    Given an ImageFolder dataset and a list of class folder names, return a dict mapping each class name to its Subset.\n",
        "    Allows us to isolate images from each genre when running the model.\n",
        "    \"\"\"\n",
        "    subsets = {}\n",
        "    for cname in class_names:\n",
        "        cidx = dataset.class_to_idx[cname]\n",
        "        indices = [i for i, (_, lbl) in enumerate(dataset.samples)\n",
        "                   if lbl == cidx]\n",
        "        subsets[cname] = Subset(dataset, indices)\n",
        "    return subsets"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FJ-Scn5TT4le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize Album Cover Images\n",
        "# @markdown For more information, see the `Full Training Pipeline.ipynb` file.\n",
        "\n",
        "# this code is lifted from the 'full training pipeline' notebook with slight modifications.\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "NOISE_VARIANCE = 0.15\n",
        "\n",
        "\n",
        "dataset_path = kagglehub.dataset_download(\"michaeljkerr/20k-album-covers-within-20-genres\")\n",
        "dataset_path = os.path.join(dataset_path, \"GAID\")\n",
        "\n",
        "copy_path = \"/content/data/albumcovers/\"\n",
        "if not os.path.exists(copy_path):\n",
        "  shutil.copytree(dataset_path, copy_path)\n",
        "dataset_path = copy_path\n",
        "\n",
        "classes_to_keep = [ \"Classical\", \"HipHop\",  \"Pop\", \"Jazz\" ]\n",
        "directories_to_delete = [f\"{dataset_path}/{name}\" for name in os.listdir(dataset_path) if name not in classes_to_keep]\n",
        "for _, dir_to_delete in enumerate(directories_to_delete):\n",
        "    shutil.rmtree(dir_to_delete)\n",
        "classes = classes_to_keep\n",
        "\n",
        "\n",
        "default_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # densenet takes 224 by 224 input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "dataset = datasets.ImageFolder(root=dataset_path)\n",
        "dataset_size = len(dataset)\n",
        "training_ratio = 0.8\n",
        "validation_ratio = 0.1\n",
        "testing_ratio = 1 - training_ratio - validation_ratio\n",
        "\n",
        "split_lengths = [int(dataset_size * training_ratio), int(dataset_size * validation_ratio), int(dataset_size * testing_ratio)]\n",
        "split_lengths[2] += dataset_size - np.sum(split_lengths)  # correct the testing dataset size\n",
        "training_dataset, validation_dataset, testing_dataset = random_split(dataset, lengths=split_lengths)\n",
        "apply_alt_training_transforms = False\n",
        "\n",
        "training_dataset.dataset = datasets.ImageFolder(root=dataset_path, transform=default_transform)\n",
        "validation_dataset.dataset = datasets.ImageFolder(root=dataset_path, transform=default_transform)\n",
        "testing_dataset.dataset = datasets.ImageFolder(root=dataset_path, transform=default_transform)\n",
        "\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, worker_init_fn=seed_worker, persistent_workers=False)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, worker_init_fn=seed_worker, persistent_workers=False)\n",
        "testing_dataloader = DataLoader(testing_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1, worker_init_fn=seed_worker, persistent_workers=False)"
      ],
      "metadata": {
        "id": "WCCCK-_Ds2eZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize Music Audio Snippets\n",
        "# @markdown For more information, see the `Full Training Pipeline.ipynb` file.\n",
        "\n",
        "# this code is lifted from the 'full training pipeline' notebook with slight modifications.\n",
        "\n",
        "SR = 22_050\n",
        "\n",
        "def load_audio_files(files, sr=SR):\n",
        "    audio_data = []\n",
        "    for file in files:\n",
        "        try:\n",
        "            data, _ = librosa.load(file, sr=sr)\n",
        "            audio_data.append(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping file {file} due to error: {e}\")\n",
        "    return audio_data\n",
        "\n",
        "def pad_audio_data(audio_data, target_length):\n",
        "    padded_audio_data = []\n",
        "    for clip in audio_data:\n",
        "        if len(clip) < target_length:  # pad if < target_length\n",
        "            pad_length = target_length - len(clip)\n",
        "            padded_clip = np.pad(clip, (0, pad_length), mode='constant')\n",
        "        else:  # truncate if > target length\n",
        "            padded_clip = clip[:target_length]\n",
        "        padded_audio_data.append(padded_clip)\n",
        "    return np.array(padded_audio_data)\n",
        "\n",
        "def get_split_dataloaders(padded_audio_data, test_size=0.2, random_state=42):\n",
        "    train_data, test_data = train_test_split(padded_audio_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize Audio Data\n",
        "    max_value = np.max([np.abs(train_data).max(), np.abs(test_data).max()])\n",
        "    train_data_normalized = train_data / max_value\n",
        "    test_data_normalized = test_data / max_value\n",
        "\n",
        "    train_tensor = torch.tensor(train_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
        "    test_tensor = torch.tensor(test_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    train_dataset = TensorDataset(train_tensor)\n",
        "    test_dataset = TensorDataset(test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
        "path = f\"{path}/Data/genres_original/\"\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "files_per_genre = [list((pathlib.Path(path) / genre.lower()).glob('*.wav')) for genre in classes]\n",
        "audio_per_genre = [load_audio_files(files) for files in files_per_genre]\n",
        "flattened = [item for sublist in audio_per_genre for item in sublist]\n",
        "target_length = min(map(len, flattened)) // 2\n",
        "audio_per_genre_padded = [pad_audio_data(audio_data, target_length) for audio_data in audio_per_genre]\n",
        "\n",
        "loaders_per_genre = [get_split_dataloaders(padded_audio) for padded_audio in audio_per_genre_padded]\n",
        "genre_loader_pairs = list(zip(classes, loaders_per_genre))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NbYq0LyMBz5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Declare Module for Classes\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional AutoEncoder for 1D signals.\n",
        "    In the context of this project, it acts as a Variational AutoEncoder (VAE) for audio waveforms.\n",
        "\n",
        "    Original Author:\n",
        "    https://yuehan-z.medium.com/introduction-to-vaes-in-ai-music-generation-d8e0cfc2245b\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input tensor.\n",
        "        out_channels (int): Number of channels in the output tensor.\n",
        "        down_channels (list[int]): Output channels for each encoder block.\n",
        "        up_channels (list[int]): Output channels for each decoder block.\n",
        "        down_rate (list[int]): Stride values for each encoder convolution.\n",
        "        up_rate (list[float]): Upsampling factors for each decoder stage.\n",
        "        cross_attention_dim (int): Number of channels in the bottleneck layers.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        down_channels,\n",
        "        up_channels,\n",
        "        down_rate,\n",
        "        up_rate,\n",
        "        cross_attention_dim\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize encoder, bottleneck, decoder, and output layers.\n",
        "        \"\"\"\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.down_channels = down_channels\n",
        "        self.up_channels = up_channels\n",
        "        self.down_rate = down_rate\n",
        "        self.up_rate = up_rate\n",
        "        self.cross_attention_dim = cross_attention_dim\n",
        "\n",
        "        # build encoder\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        for i, out_ch in enumerate(down_channels):\n",
        "            in_ch = in_channels if i == 0 else down_channels[i-1]\n",
        "            stride = down_rate[i]\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1),\n",
        "                nn.BatchNorm1d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm1d(out_ch),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            self.encoder_layers.append(layer)\n",
        "\n",
        "        # build bottleneck\n",
        "        self.bottleneck_layers = nn.Sequential(\n",
        "            nn.Conv1d(down_channels[-1], cross_attention_dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(cross_attention_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(cross_attention_dim, cross_attention_dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(cross_attention_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # build decoder\n",
        "        self.decoder_layers = nn.ModuleList()\n",
        "        for i, out_ch in enumerate(up_channels):\n",
        "            in_ch = cross_attention_dim if i == 0 else up_channels[i-1]\n",
        "            layer = nn.Sequential(\n",
        "                nn.ConvTranspose1d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.ConvTranspose1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm1d(out_ch),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            self.decoder_layers.append(layer)\n",
        "\n",
        "        # final output\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.ConvTranspose1d(up_channels[-1], out_channels, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the autoencoder.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (B, in_channels, L).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reconstructed tensor of shape (B, out_channels, 330000).\n",
        "        \"\"\"\n",
        "        x = self.encode(x)\n",
        "        x = self.bottleneck(x)\n",
        "        x = self.decode(x)\n",
        "        x_out = self.output_layer(x)\n",
        "        # trim or pad to fixed length\n",
        "        x_out = x_out[:, :, :330_000]\n",
        "        return x_out\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Apply encoder layers to downsample input.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Latent representation.\n",
        "        \"\"\"\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def bottleneck(self, x):\n",
        "        \"\"\"\n",
        "        Process latent tensor through bottleneck convolutional blocks.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Encoder output tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Features at bottleneck.\n",
        "        \"\"\"\n",
        "        return self.bottleneck_layers(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        \"\"\"\n",
        "        Apply decoder layers with upsampling to reconstruct features.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Bottleneck feature tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Upsampled feature tensor before output.\n",
        "        \"\"\"\n",
        "        for i, layer in enumerate(self.decoder_layers):\n",
        "            target_length = int(x.shape[2] * self.up_rate[i])\n",
        "            x = nn.functional.interpolate(x, size=target_length, mode='nearest')\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Img2Music(nn.Module):\n",
        "    \"\"\"\n",
        "    Image-to-Music generator combining a ConvNeXt classifier with per-genre VAEs.\n",
        "\n",
        "    Args:\n",
        "        weights_dir (str):             Directory path containing classifier and VAE pre-trainined weights.\n",
        "        genre_audios (list[list]):     Lists of raw audio samples per genre.\n",
        "        classes (list[str]):           Names of the music genres for VAEs.\n",
        "        img_features_weight (float):   Influence of image features on generated audio.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        weights_dir: str,\n",
        "        genre_audios: list,\n",
        "        classes: list,\n",
        "        img_features_weight: float = 1e-3,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize ConvNeXt classifier and per-genre VAEs.\n",
        "\n",
        "        - Sets up a ConvNeXt_Base pretrained on ImageNet, replaces the final linear\n",
        "          layer to match genre count, and loads its weights from weights_dir.\n",
        "        - Instantiates a VAE for each genre and loads its weights.\n",
        "        \"\"\"\n",
        "        super(Img2Music, self).__init__()\n",
        "        self.genre_audios = genre_audios\n",
        "        self.img_features_weight = img_features_weight\n",
        "\n",
        "        # Load and configure ConvNeXt classifier\n",
        "        weights = torchvision.models.convnext.ConvNeXt_Base_Weights.IMAGENET1K_V1\n",
        "        convnext = torchvision.models.convnext_base(weights=weights)\n",
        "        convnext.classifier[2] = nn.Linear(in_features=1024, out_features=len(classes), bias=True)\n",
        "        convnext.load_state_dict(torch.load(f\"{weights_dir}classifier.pth\"))\n",
        "        self.classifier = convnext\n",
        "\n",
        "        # Load per-genre AutoEncoders\n",
        "        model_cfg = {\n",
        "            'in_channels': 1,\n",
        "            'out_channels': 1,\n",
        "            'down_channels': [16, 32, 64, 128],\n",
        "            'up_channels': [384, 192, 96, 48],\n",
        "            'down_rate': [4, 4, 3, 2],\n",
        "            'up_rate': [2, 3, 4, 4],\n",
        "            'cross_attention_dim': 1024,\n",
        "        }\n",
        "        self.vaes = []\n",
        "        for cls_name in classes:\n",
        "            vae = AutoEncoder(**model_cfg)\n",
        "            vae.load_state_dict(torch.load(f\"{weights_dir}{cls_name.lower()}.pth\"))\n",
        "            vae.to(DEVICE)\n",
        "            vae.eval()\n",
        "            self.vaes.append(vae)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[int, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Generate a music sample from an input image.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input image tensor of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            genre (int): Predicted genre index.\n",
        "            audio (Tensor): Generated audio waveform tensor.\n",
        "        \"\"\"\n",
        "        # predict genre and select corresponding VAE\n",
        "        genre = self.classifier(x).argmax(dim=1).item()\n",
        "        vae = self.vaes[genre]\n",
        "\n",
        "        # pick a random seed audio and reshape\n",
        "        sample = random.choice(self.genre_audios[genre])\n",
        "        sample = torch.tensor(sample).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
        "        sample_embed = vae.bottleneck(vae.encode(sample))\n",
        "\n",
        "        # add image feature vector to sampled embedding vector\n",
        "        x_gray = x.mean(dim=1, keepdim=True)\n",
        "        feature_vec = torch.nn.functional.interpolate(x_gray, size=(1024, 3438), mode='bilinear', align_corners=False)\n",
        "        feature_vec = feature_vec.squeeze(1).to(DEVICE)\n",
        "        feature_vec = feature_vec * self.img_features_weight\n",
        "\n",
        "        # generate new audio via VAE\n",
        "        decoded = vae.output_layer(vae.decode(sample_embed.to(DEVICE) + feature_vec.to(DEVICE)))\n",
        "        audio = decoded.squeeze(0).squeeze(0)\n",
        "        return genre, audio\n",
        "\n",
        "\n",
        "model = Img2Music('/content/pretrained_weights/', audio_per_genre_padded, classes)\n",
        "model = model.to(DEVICE)\n",
        "model.eval();\n",
        "\n",
        "alt_dataset = datasets.ImageFolder(root=dataset_path, transform=default_transform)\n",
        "classes = [\"Classical\", \"HipHop\", \"Pop\", \"Jazz\"]\n",
        "album_covers = get_class_subsets(alt_dataset, classes)"
      ],
      "metadata": {
        "id": "oF3RZHBS9RsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the model"
      ],
      "metadata": {
        "id": "hT8_zrSTTq5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample from dataset\n",
        "# @markdown Here, we pass an album cover image from the dataset.\n",
        "# @markdown Configure the below values to select which album cover image from which genre to use.\n",
        "# @markdown ****\n",
        "\n",
        "# setup\n",
        "genre = \"Jazz\" # @param [\"Classical\", \"HipHop\", \"Pop\", \"Jazz\"]\n",
        "genre_idx = classes.index(genre)\n",
        "\n",
        "idx = 0 # @param {type:\"integer\"}\n",
        "idx = idx % 1000\n",
        "idx = max(0, min(idx, 1000-1))\n",
        "\n",
        "album_cover, _ = album_covers[genre][idx]\n",
        "album_cover = album_cover.unsqueeze(0)\n",
        "\n",
        "\n",
        "# display the input album cover image\n",
        "img = album_cover.squeeze(0).permute(1, 2, 0).numpy()\n",
        "img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(f'Input Album Cover Image ({classes[genre_idx]})')\n",
        "\n",
        "\n",
        "# pass it through the model\n",
        "pred_genre, audio = model(album_cover.to(DEVICE))\n",
        "audio_np = audio.detach().cpu().numpy()\n",
        "print(f\"Predicted genre of input image: \\\"{classes[pred_genre]}\\\"\")\n",
        "display(Audio(audio_np, rate=SR))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eNLnzYXq-2aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom Sample\n",
        "# @markdown Here, we can upload our own image and pass it through the model.\n",
        "# @markdown The classifier will predict what the most likely genre of the image is, then generate audio corresponding to the specific genre.\n",
        "\n",
        "# @markdown Simply run this code block to start.\n",
        "\n",
        "# setup\n",
        "uploaded = files.upload()  # displays file picker\n",
        "fname, data = next(iter(uploaded.items()))\n",
        "img = Image.open(BytesIO(data)).convert(\"RGB\")\n",
        "tensor = default_transform(img)\n",
        "tensor = tensor.unsqueeze(0)\n",
        "\n",
        "\n",
        "# display the input image\n",
        "img = tensor.squeeze(0).permute(1, 2, 0).numpy()\n",
        "img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(f'Input image:')\n",
        "\n",
        "\n",
        "# pass it through the model\n",
        "pred_genre, audio = model(tensor.to(DEVICE))\n",
        "audio_np = audio.detach().cpu().numpy()\n",
        "print(f\"\\n\\nPredicted genre of input image: \\\"{classes[pred_genre]}\\\"\")\n",
        "display(Audio(audio_np, rate=SR))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4Cgpq3aVOjEQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}